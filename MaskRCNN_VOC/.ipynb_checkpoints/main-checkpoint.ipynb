{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tutorials.pytorch.kr/intermediate/torchvision_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/vision/blob/main/torchvision/datasets/voc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pytorch - MaskRCNN\n",
    "https://github.com/pytorch/vision/blob/main/torchvision/models/detection/mask_rcnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impoart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from MaskRCNN_VOC.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import MaskRCNN_VOC as mrv\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import bbox_visualizer as bbv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset (Pascal VOC 2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Resize([100, 100])\n",
    "    \n",
    "])\n",
    "target_transforms = torchvision.transforms.Compose([\n",
    "])\n",
    "\n",
    "dataset = mrv.VOC_Dataset(root = \"/home\", image_set = \"train\", option=2)\n",
    "data_loader = mrv.VOC_Dataloader(dataset, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "model.roi_heads.box_predictor.cls_score = torch.nn.Linear(in_features=1024, out_features=21, bias=True)\n",
    "model.roi_heads.box_predictor.bbox_pred = torch.nn.Linear(in_features=1024, out_features=84, bias=True)\n",
    "model.roi_heads.mask_predictor.mask_fcn_logits = torch.nn.Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"my_trained_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    global a, b\n",
    "    model.train()\n",
    "#     metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "#     metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "#     header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "#     for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "    losses_list = []\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        print(\"0\")\n",
    "        optimizer.zero_grad()\n",
    "        print(\"1\")\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "#             losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_list = torch.stack([value for value in loss_dict.values()])\n",
    "            losses = loss_list @ loss_list**(1/2)\n",
    "            losses_list.append(losses)\n",
    "        print(\"2\")\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "#         loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        \n",
    "#         losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "#         losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "#         loss_value = losses_reduced.item()\n",
    "\n",
    "#         if not math.isfinite(loss_value):\n",
    "#             print(f\"Loss is {loss_value}, stopping training\")\n",
    "# #             print(loss_dict_reduced)\n",
    "#             sys.exit(1)\n",
    "        print(\"3\")\n",
    "        try:\n",
    "            if scaler is not None:\n",
    "                scaler.scale(losses).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                print(\"a\")\n",
    "                losses.backward()\n",
    "                print(\"b\")\n",
    "                optimizer.step()\n",
    "                print(\"c\")\n",
    "        except:\n",
    "            a = images\n",
    "            b = targets\n",
    "        print(\"4\")\n",
    "        try:\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "        except:\n",
    "            a = images\n",
    "            b = targets\n",
    "        print(losses)\n",
    "    return torch.mean(torch.stack(losses_list)).to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(5.7957, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(5.3154, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(4.5546, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(3.7383, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(3.1748, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(2.1697, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.9205, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.0896, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.0337, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.1665, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.0802, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.0360, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(0.8046, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.0201, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.0941, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(0.8429, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(0.8033, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(0.8920, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.0439, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(0.7628, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(0.8925, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(0.9521, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.2562, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(1.1141, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "b\n",
      "c\n",
      "4\n",
      "tensor(0.9041, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "a\n",
      "4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "numel: integer multiplication overflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(avg_loss)\n",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     63\u001b[0m         a \u001b[38;5;241m=\u001b[39m images\n\u001b[1;32m     64\u001b[0m         b \u001b[38;5;241m=\u001b[39m targets\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mstack(losses_list))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:426\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    423\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66m# All strings are unicode in Python 3.\u001b[39m\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py:637\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    636\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py:568\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    566\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    567\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 568\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    571\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py:328\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py:115\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;66m# no valid number, do nothing\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: numel: integer multiplication overflow"
     ]
    }
   ],
   "source": [
    "epoch = 300\n",
    "for e in range(epoch):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    avg_loss = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq = 10)\n",
    "    print(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_trained_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
