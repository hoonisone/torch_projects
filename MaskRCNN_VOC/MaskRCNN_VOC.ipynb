{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ece929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import bbox_visualizer as bbv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torchvision.datasets.voc import *\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "487fc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "VOC_LABEL = [   'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus','car','cat','chair','cow',\n",
    "                'diningtable', 'dog','horse','motorbike', 'person','pottedplant', 'sheep','sofa','train','tvmonitor']\n",
    "VOC_LABEL_PAIR = {VOC_LABEL[i]:i+1 for i in range(len(VOC_LABEL))}\n",
    "\n",
    "\n",
    "WRONG_FILE_NAMES_IN_TRAIN = ['2009_005069']\n",
    "WRONG_FILES_NAMES_IN_VAL = ['2008_005245', '2009_000455', '2009_004969', '2011_002644', '2011_002863']\n",
    "WRONG_FILES_NAMES = WRONG_FILE_NAMES_IN_TRAIN+WRONG_FILES_NAMES_IN_VAL\n",
    "def remove_wrong_annotated_file(file_names):\n",
    "    for wrong_file_name in WRONG_FILES_NAMES:\n",
    "        if wrong_file_name in file_names:\n",
    "            file_names.remove(wrong_file_name)\n",
    "    return file_names\n",
    "\n",
    "\n",
    "class VOC2012_MaskRCNN_InstanceSegmentation_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        image_set: str = \"train\",\n",
    "        image_transform: Optional[Callable] = Compose([torchvision.transforms.ToTensor()]),\n",
    "        label_transform: Optional[Callable] = None,\n",
    "        box_transform: Optional[Callable] = None,\n",
    "        mask_transform: Optional[Callable] = None\n",
    "\n",
    "    ):\n",
    "        root = Path(root)\n",
    "        self.image_transform = image_transform\n",
    "        self.label_transform = label_transform\n",
    "        self.box_transform = box_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "\n",
    "        self.image_set = verify_str_arg(image_set, \"image_set\", [\"train\", \"trainval\", \"val\"])\n",
    "\n",
    "        split_f = root/\"ImageSets\"/\"Segmentation\"/f\"{image_set}.txt\"\n",
    "        with open(os.path.join(split_f)) as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "            file_names = remove_wrong_annotated_file(file_names)\n",
    "            \n",
    "        # dir setting\n",
    "        image_dir = root/\"JPEGImages\"\n",
    "        annotation_dir = root/\"Annotations\"\n",
    "        mask_dir = root/\"SegmentationObject\"\n",
    "\n",
    "        # file name list setting\n",
    "        self.images = [image_dir/f\"{x}.jpg\" for x in file_names]\n",
    "        self.annotations = [annotation_dir/f\"{x}.xml\" for x in file_names]\n",
    "        self.masks = [mask_dir/f\"{x}.png\" for x in file_names]\n",
    "        \n",
    "        assert len(self.images) == len(self.annotations) == len(self.masks)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        image = self.get_image(index)\n",
    "        labels, boxes = self.get_annotation(index)\n",
    "        masks = self.get_masks(index)\n",
    "\n",
    "        target = {\n",
    "            \"labels\": torch.tensor(labels),\n",
    "            \"boxes\" : torch.tensor(boxes),\n",
    "            \"masks\" : torch.tensor(masks)\n",
    "        }\n",
    "        \n",
    "        return (image, target)\n",
    "    \n",
    "    def get_image(self, index):\n",
    "        image = Image.open(self.images[index]).convert(\"RGB\")\n",
    "        if self.image_transform is not None:\n",
    "            image = self.image_transform(image)\n",
    "        return image\n",
    "    \n",
    "    def get_masks(self, index):\n",
    "        mask = Image.open(self.masks[index])\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        id_list = np.unique(mask)\n",
    "        id_list = np.delete(id_list, np.where((id_list == 0) | (id_list == 255)))\n",
    "\n",
    "        masks = (mask[None, :] == id_list.reshape(-1, 1, 1)).astype(np.uint8)\n",
    "    \n",
    "        if self.mask_transform is not None:\n",
    "            masks = self.image_transform(masks)\n",
    "\n",
    "        return masks\n",
    "    \n",
    "    def get_annotation(self, index):# labels, boxes\n",
    "        annotation = VOCDetection.parse_voc_xml(ET_parse(self.annotations[index]).getroot())\n",
    "        objects = annotation[\"annotation\"][\"object\"]\n",
    "        \n",
    "        labels = np.array([VOC_LABEL_PAIR[o[\"name\"]] for o in objects], dtype = np.int64)\n",
    "        boxes = [o[\"bndbox\"] for o in objects]\n",
    "        boxes = np.array([[box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]] for box in boxes], dtype = np.float32)\n",
    "\n",
    "        if self.label_transform is not None:\n",
    "            labels = self.label_transform(labels)\n",
    "        \n",
    "        if self.box_transform is not None:\n",
    "            boxes = self.box_transform(boxes)\n",
    "        \n",
    "        return (labels, boxes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6c4c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2012_MaskRCNN_InstanceSegmentation:\n",
    "    CLASS_NUM = 21\n",
    "    SCORE_THRESHOLDS = [0.1*i for i in range(1, 11)]\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_eval(model, device, data_loader):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        loss_list = []\n",
    "        for images, targets in tqdm(data_loader):\n",
    "            images, targets = __class__.batch_to(images, targets, device)\n",
    "            loss_dict = model(images, targets)\n",
    "            loss_vector = torch.stack([value for value in loss_dict.values()])\n",
    "            loss = loss_vector @ loss_vector**(1/2)\n",
    "            loss_list.append(loss)\n",
    "        return np.array(loss_list).mean()\n",
    "        \n",
    "    def map_eval(model, device, data_loader, class_num, mask_threshold, score_threshold):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        counts = []\n",
    "        for images, targets in tqdm(data_loader):\n",
    "            images, targets = __class__.batch_to(images, targets, device)\n",
    "            results = model(images, targets)\n",
    "\n",
    "            # 클래스 별로 GT, TP, Detection 개수 카운트하여 counts에 추가\n",
    "            for result, target in zip(results, targets):\n",
    "\n",
    "                result = __class__.tensor_to_numpy_for_result(result)\n",
    "                target = __class__.tensor_to_numpy_for_target(target)\n",
    "\n",
    "                p_scores, p_labels, p_boxes, p_masks = __class__.dict_to_tuple_for_result(result)\n",
    "                gt_labels, gt_boxes, gt_masks = __class__.dict_to_tuple_for_target(target)\n",
    "\n",
    "                p_masks = __class__.to_binary_by_threshold(p_masks, threshold = mask_threshold).squeeze(1)\n",
    "\n",
    "\n",
    "                count_result = __class__.count_result(p_masks, p_labels, p_scores, gt_masks, gt_labels, \n",
    "                                            class_num = class_num, \n",
    "                                            score_thresholds = score_threshold,\n",
    "                                            iou_threshold = 0.3)\n",
    "                counts.append(count_result)\n",
    "        \n",
    "        # 카운트 결과 종합\n",
    "        detected_num_list = [count[\"detected_num\"] for count in counts]\n",
    "        gt_num_list = [count[\"gt_num\"] for count in counts]\n",
    "        tp_num_list = [count[\"tp_num\"] for count in counts]\n",
    "\n",
    "        detected_num = np.stack(detected_num_list).sum(0)\n",
    "        gt_num = np.stack(gt_num_list).sum(0)\n",
    "        tp_num = np.stack(tp_num_list).sum(0)\n",
    "\n",
    "        # MAP 계산 후 반환\n",
    "        return __class__.ap_per_class(detected_num, gt_num, tp_num)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_to_numpy(data):\n",
    "        return data.to(\"cpu\").detach().numpy()\n",
    "    \n",
    "    # Batch #######################################################\n",
    "    @staticmethod\n",
    "    def batch_to(images, targets, device):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k:v.to(device) for k, v in t.items()} for t in targets]\n",
    "        return images, targets\n",
    "\n",
    "    # Image #######################################################\n",
    "    @staticmethod\n",
    "    def tensor_to_numpy_for_image(tensor_image):\n",
    "        return __class__.tensor_to_numpy(tensor_image).transpose((1, 2, 0))\n",
    "    \n",
    "    # Target    \n",
    "    @staticmethod\n",
    "    def dict_to_tuple_for_target(sample_target):\n",
    "        return sample_target[\"labels\"], sample_target[\"boxes\"], sample_target[\"masks\"]\n",
    "    \n",
    "    @staticmethod\n",
    "    def tuple_to_dict_for_target(labels, boxes, masks):\n",
    "        return {\"labels\":labels, \"boxes\":boxes, \"masks\":masks}\n",
    "    \n",
    "    @staticmethod \n",
    "    def tensor_to_numpy_for_target(sample_target):\n",
    "        labels, boxes, masks = __class__.dict_to_tuple_for_target(sample_target)\n",
    "        labels = __class__.tensor_to_numpy(labels)\n",
    "        boxes = __class__.tensor_to_numpy(boxes)\n",
    "        masks = __class__.tensor_to_numpy(masks)\n",
    "        return __class__.tuple_to_dict_for_target(labels, boxes, masks)\n",
    "    \n",
    "    # Result #######################################################\n",
    "    @staticmethod\n",
    "    def dict_to_tuple_for_result(sample_result):\n",
    "        return  sample_result[\"scores\"], sample_result[\"labels\"], sample_result[\"boxes\"], sample_result[\"masks\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def tuple_to_dict_for_result(scores, labels, boxes, masks):\n",
    "        return {\"scores\":scores, \"labels\":labels, \"boxes\":boxes, \"masks\":masks}\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_to_numpy_for_result(sample_result):\n",
    "        scores, labels, boxes, masks = __class__.dict_to_tuple_for_result(sample_result)\n",
    "        scores = __class__.tensor_to_numpy(scores)\n",
    "        labels = __class__.tensor_to_numpy(labels)\n",
    "        boxes = __class__.tensor_to_numpy(boxes)\n",
    "        masks = __class__.tensor_to_numpy(masks)\n",
    "        return __class__.tuple_to_dict_for_result(scores, labels, boxes, masks)\n",
    "    \n",
    "    @staticmethod\n",
    "    def show_sample_result(sample_result):\n",
    "        scores, labels, boxes, masks = __class__.dict_to_tuple_for_result(sample_result)\n",
    "        \n",
    "        titles = [f\"{VOC_LABEL[l-1]}({round(s.item(), 2)})\" for l, s in zip(labels, scores)]\n",
    "        __class__.show_images(masks, titles)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_sample_by_score(sample_result, threshold):\n",
    "        scores, labels, boxes, masks = __class__.dict_to_tuple_for_result(sample_result)\n",
    "        indexes =  np.where(scores>threshold)[0]\n",
    "        return __class__.tuple_to_dict_for_result(scores[indexes], labels[indexes], boxes[indexes], masks[indexes])\n",
    "\n",
    "    @staticmethod\n",
    "    def to_binary_by_threshold(data, threshold):\n",
    "        \"\"\"\n",
    "        Numpy mask: binary 데이터로 바뀔 대상 \n",
    "        threshold: binary 기준 (threshold보다 크면 1 아니면 0)\n",
    "        \"\"\"\n",
    "        return (data > threshold).astype(bool)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def show_images(images, titles):\n",
    "        n = len(images) + 1\n",
    "        fig = plt.figure(figsize = (4, 4*n))\n",
    "        for i, (image, title) in enumerate(zip(images, titles)):\n",
    "            sub = fig.add_subplot(n, 1, i+1)\n",
    "            sub.imshow(image)\n",
    "            sub.set_title(title)\n",
    "\n",
    "        return fig\n",
    "    \n",
    "    # Evaluation\n",
    "    @staticmethod\n",
    "    def score_level(score, thresholds):\n",
    "        for level, threshold in enumerate(thresholds):\n",
    "            if score <= threshold:\n",
    "                return level\n",
    "    \n",
    "    @staticmethod\n",
    "    def iou(mask1, mask2):\n",
    "        intersection = (mask1 * mask2).sum()\n",
    "        union = (mask1 + mask2).sum() - intersection\n",
    "        return intersection/union\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_result(p_masks, p_labels, p_scores, gt_masks, gt_labels, class_num, score_thresholds, iou_threshold):\n",
    "        \"\"\"\n",
    "            detection 결과에 대해 다음 샘플의 개수를 계산한다.\n",
    "            - ground truth object num per class\n",
    "            - detected object num per class\n",
    "            - positive num per class and score level\n",
    "\n",
    "            score level은 detection 결과에 대한 분류 확률을 score threshold로 cut할 때 몇 번째 구간에 들어가는가를 의미 (높을수록 정확)\n",
    "        \"\"\"\n",
    "        score_levels = [__class__.score_level(score, score_thresholds) for score in p_scores]\n",
    "        \n",
    "        #########################################################################\n",
    "        # prediction과 GT의 idx를 class 별로 구분\n",
    "        # 클래스 별 detected mask indexes\n",
    "        detected_indexes_per_c = [[] for i in range(class_num)]\n",
    "        for idx, label in enumerate(p_labels):\n",
    "            detected_indexes_per_c[label].append(idx)\n",
    "\n",
    "        gt_indexes_per_c = [[] for i in range(class_num)]\n",
    "        for idx, label in enumerate(gt_labels):\n",
    "            gt_indexes_per_c[label].append(idx)\n",
    "            \n",
    "        #########################################################################\n",
    "        # prediction과 GT를 class별로 카운트\n",
    "        \n",
    "        # 클래스 별 탐지한 object 개수\n",
    "        detected_num = np.array([len(x) for x in detected_indexes_per_c])\n",
    "\n",
    "        # 클래스 별 object 개수\n",
    "        gt_num = np.array([len(x) for x in gt_indexes_per_c])\n",
    "        \n",
    "\n",
    "        #########################################################################\n",
    "        \n",
    "\n",
    "        tp_num = np.zeros((class_num, len(score_thresholds)))\n",
    "\n",
    "        for c in range(class_num):\n",
    "            p_idxes = detected_indexes_per_c[c]\n",
    "            gt_idxes = gt_indexes_per_c[c]\n",
    "\n",
    "            if (not p_idxes) or (not gt_idxes):\n",
    "                continue\n",
    "            \n",
    "            for p_idx in p_idxes:\n",
    "                if not gt_idxes: # 탐색할게 없으면 끝\n",
    "                    break\n",
    "\n",
    "                iou_list = [__class__.iou(p_masks[p_idx], gt_masks[gt_idx]) for gt_idx in gt_idxes]\n",
    "\n",
    "                max_iou = max(iou_list)\n",
    "                max_idx = np.array(iou_list).argmax()\n",
    "\n",
    "                if iou_threshold <= max_iou:\n",
    "                    tp_num[c][score_levels[p_idx]] += 1\n",
    "                    gt_idxes.pop(max_idx)\n",
    "                    \n",
    "\n",
    "        return {\n",
    "            \"detected_num\":detected_num,\n",
    "            \"gt_num\":gt_num,\n",
    "            \"tp_num\":tp_num\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def ap(detected_num, gt_num, tp_num):\n",
    "        if gt_num == 0:\n",
    "            return 0\n",
    "\n",
    "        for i in range(len(tp_num)-1, 0, -1):\n",
    "            tp_num[i-1] += tp_num[i]\n",
    "\n",
    "        \n",
    "        recall = tp_num/gt_num\n",
    "        precision = tp_num/detected_num\n",
    "        \n",
    "        # precision을 계단식으로 맞추기\n",
    "        for i in range(1, len(precision)):\n",
    "            precision[i] = max(precision[i-1], precision[i])\n",
    "        \n",
    "        return (recall * precision).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def ap_per_class(detected_num, gt_num, tp_num):\n",
    "        ap_list = []\n",
    "        for c in range(len(detected_num)):\n",
    "            ap_value = __class__.ap(detected_num[c], gt_num[c], tp_num[c])\n",
    "            ap_list.append(ap_value)\n",
    "\n",
    "        return np.array(ap_list).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    images = [sample[0] for sample in batch]\n",
    "    targets = [sample[1] for sample in batch]\n",
    "    \n",
    "    return images, targets\n",
    "\n",
    "class VOC2012_MaskRCNN_InstanceSegmentation_DataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        super().__init__(dataset, batch_size = batch_size, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26ada6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train()\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    losses_list = []\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            loss_list = torch.stack([value for value in loss_dict.values()])\n",
    "            losses = loss_list @ loss_list**(1/2)\n",
    "            losses_list.append(losses)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    return torch.mean(torch.stack(losses_list)).to(\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
