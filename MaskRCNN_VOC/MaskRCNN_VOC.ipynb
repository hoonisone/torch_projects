{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ece929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from mh_utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "import torchvision\n",
    "import mh_utils as MH\n",
    "\n",
    "import numpy as np\n",
    "import bbox_visualizer as bbv\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torchvision.datasets.voc import *\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22409e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Device:\n",
    "    # Input data\n",
    "    @staticmethod\n",
    "    def to_for_sample_data(sample_data, device):\n",
    "        image, target = sample_data\n",
    "        image = image.detach().to(device)\n",
    "        target = {k:v.detach().to(device) for k, v in target.items()}\n",
    "        return (image, target)\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_for_batch_data(batch_data, device):\n",
    "        images, targets = batch_data\n",
    "        images = [image.detach().to(device) for image in images]\n",
    "        targets = [{k:v.detach().to(device) for k, v in t.items()} for t in targets]\n",
    "        return (images, targets)\n",
    "\n",
    "    # Result\n",
    "    @staticmethod\n",
    "    def to_for_sample_result(sample_result, device):\n",
    "        return {k:v.detach().to(device) for k, v in sample_result.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def to_for_batch_result(batch_result, device):\n",
    "        return [__class__.to_for_sample_result(x, device) for x in batch_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487fc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "VOC_LABEL = [   \"background\", 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus','car','cat','chair','cow',\n",
    "                'diningtable', 'dog','horse','motorbike', 'person','pottedplant', 'sheep','sofa','train','tvmonitor']\n",
    "VOC_LABEL_PAIR = {VOC_LABEL[i]:i for i in range(len(VOC_LABEL))}\n",
    "\n",
    "\n",
    "WRONG_FILE_NAMES_IN_TRAIN = ['2009_005069']\n",
    "WRONG_FILES_NAMES_IN_VAL = ['2008_005245', '2009_000455', '2009_004969', '2011_002644', '2011_002863']\n",
    "WRONG_FILES_NAMES = WRONG_FILE_NAMES_IN_TRAIN+WRONG_FILES_NAMES_IN_VAL\n",
    "def remove_wrong_annotated_file(file_names):\n",
    "    for wrong_file_name in WRONG_FILES_NAMES:\n",
    "        if wrong_file_name in file_names:\n",
    "            file_names.remove(wrong_file_name)\n",
    "    return file_names\n",
    "\n",
    "\n",
    "class VOC2012_MaskRCNN_InstanceSegmentation_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        image_set: str = \"train\",\n",
    "        image_transform: Optional[Callable] = Compose([torchvision.transforms.ToTensor()]),\n",
    "        label_transform: Optional[Callable] = None,\n",
    "        box_transform: Optional[Callable] = None,\n",
    "        mask_transform: Optional[Callable] = None,\n",
    "        cropping = False,\n",
    "        mask_expending = False,\n",
    "        flip = False,\n",
    "        jitter = False\n",
    "\n",
    "\n",
    "    ):\n",
    "        root = Path(root)\n",
    "        self.image_transform = image_transform\n",
    "        self.label_transform = label_transform\n",
    "        self.box_transform = box_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.cropping = cropping\n",
    "        self.mask_expending = mask_expending\n",
    "        self.flip = flip\n",
    "        self.jitter = jitter\n",
    "\n",
    "\n",
    "        self.image_set = verify_str_arg(image_set, \"image_set\", [\"train\", \"trainval\", \"val\"])\n",
    "\n",
    "        split_f = root/\"ImageSets\"/\"Segmentation\"/f\"{image_set}.txt\"\n",
    "        with open(os.path.join(split_f)) as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "            file_names = remove_wrong_annotated_file(file_names)\n",
    "            \n",
    "        # dir setting\n",
    "        image_dir = root/\"JPEGImages\"\n",
    "        annotation_dir = root/\"Annotations\"\n",
    "        mask_dir = root/\"SegmentationObject\"\n",
    "\n",
    "        # file name list setting\n",
    "        self.images = [image_dir/f\"{x}.jpg\" for x in file_names]\n",
    "        self.annotations = [annotation_dir/f\"{x}.xml\" for x in file_names]\n",
    "        self.masks = [mask_dir/f\"{x}.png\" for x in file_names]\n",
    "        \n",
    "        assert len(self.images) == len(self.annotations) == len(self.masks)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        image = self.get_image(index)\n",
    "        labels, boxes = self.get_annotation(index)\n",
    "        masks = self.get_masks(index)\n",
    "        \n",
    "        labels = torch.tensor(labels)\n",
    "        boxes = torch.tensor(boxes)\n",
    "        masks = torch.tensor(masks)\n",
    "\n",
    "        # random_cropping ###########################################\n",
    "        if self.cropping:\n",
    "            c, h, w = image.shape\n",
    "            image, masks = MH.random_cropping(image, masks, size=(min(200, h), min(200, w)))\n",
    "            # mask에 아직 객체의 mask가 남아있는가?\n",
    "            is_instance = masks.reshape(masks.shape[0], -1).any(1) == True\n",
    "            # indexes에 해당하는 것만 뽑아서 mask list로 형태 복원\n",
    "            boxes = torch.stack([MH.extract_box_from_binary_mask(mask) for mask in masks])\n",
    "\n",
    "            # boxes = torch.zeros(boxes.shape)\n",
    "            # boxes = torch.tensor([[0, 0, 1, 1] for i in range(boxes.shape[0])])\n",
    "            labels = labels * is_instance.type(torch.uint8)\n",
    "        ###########################################\n",
    "\n",
    "        # expend mask ###########################################\n",
    "        # 각 마스크를 상하좌우 1픽셀식 확장\n",
    "        if self.mask_expending:\n",
    "            masks = [torch.tensor(MH.expend_mask(mask.numpy(), 2)) for mask in masks]\n",
    "            masks = torch.stack(masks)\n",
    "        ###########################################\n",
    "\n",
    "        # Flip #########################################\n",
    "\n",
    "        if self.flip:\n",
    "            if np.random.rand() < 0.5:\n",
    "                image = torchvision.transforms.RandomHorizontalFlip(p=1)(image)\n",
    "                masks = torchvision.transforms.RandomHorizontalFlip(p=1)(masks)\n",
    "            \n",
    "            if np.random.rand() < 0.5:\n",
    "                image = torchvision.transforms.RandomVerticalFlip(p=1)(image)\n",
    "                masks = torchvision.transforms.RandomVerticalFlip(p=1)(masks)\n",
    "            \n",
    "        if self.jitter:\n",
    "            image = torchvision.transforms.ColorJitter(brightness=(0.3, 3), contrast=(0.45, 0.55), saturation=(0.45, 0.55), hue=(0.45, 0.45))(image)\n",
    "        target = {\n",
    "            \"labels\": labels,\n",
    "            \"boxes\" : boxes,\n",
    "            \"masks\" : masks\n",
    "        }\n",
    "        \n",
    "        return (image, target)\n",
    "\n",
    "    \n",
    "    def get_image(self, index):\n",
    "        image = Image.open(self.images[index]).convert(\"RGB\")\n",
    "        if self.image_transform is not None:\n",
    "            image = self.image_transform(image)\n",
    "        return image\n",
    "    \n",
    "    def get_masks(self, index):\n",
    "        mask = Image.open(self.masks[index])\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        id_list = np.unique(mask)\n",
    "        id_list = np.delete(id_list, np.where((id_list == 0) | (id_list == 255)))\n",
    "\n",
    "        masks = (mask[None, :] == id_list.reshape(-1, 1, 1)).astype(np.uint8)\n",
    "    \n",
    "        if self.mask_transform is not None:\n",
    "            masks = self.image_transform(masks)\n",
    "\n",
    "        return masks\n",
    "    \n",
    "    def get_annotation(self, index):# labels, boxes\n",
    "        annotation = VOCDetection.parse_voc_xml(ET_parse(self.annotations[index]).getroot())\n",
    "        objects = annotation[\"annotation\"][\"object\"]\n",
    "        \n",
    "        labels = np.array([VOC_LABEL_PAIR[o[\"name\"]] for o in objects], dtype = np.int64)\n",
    "        boxes = [o[\"bndbox\"] for o in objects]\n",
    "        boxes = np.array([[box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]] for box in boxes], dtype = np.float32)\n",
    "\n",
    "        if self.label_transform is not None:\n",
    "            labels = self.label_transform(labels)\n",
    "        \n",
    "        if self.box_transform is not None:\n",
    "            boxes = self.box_transform(boxes)\n",
    "        \n",
    "        return (labels, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681f7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2012_MaskRCNN_InstanceSegmentation_DataLoader(torch.utils.data.DataLoader):\n",
    "    @staticmethod\n",
    "    def collate_batch(batch):\n",
    "        images = [sample[0] for sample in batch]\n",
    "        targets = [sample[1] for sample in batch]\n",
    "        \n",
    "        return images, targets\n",
    "\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        super().__init__(dataset, batch_size = batch_size, collate_fn=__class__.collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6c4c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2012_MaskRCNN_InstanceSegmentation:\n",
    "    CLASS_NUM = 21\n",
    "    SCORE_THRESHOLDS = [0.1*i for i in range(10, 0, -1)]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model(pre_trained_model_path = \"\"):\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "        model.roi_heads.box_predictor.cls_score = torch.nn.Linear(in_features=1024, out_features=21, bias=True)\n",
    "        model.roi_heads.box_predictor.bbox_pred = torch.nn.Linear(in_features=1024, out_features=84, bias=True)\n",
    "        model.roi_heads.mask_predictor.mask_fcn_logits = torch.nn.Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
    "        if pre_trained_model_path:\n",
    "            model.load_state_dict(torch.load(pre_trained_model_path))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def show_comparison_for_one_sample(model, sample, device, iou_threshold = 0.5, mask_binary_threshold=0.5):\n",
    "        \"\"\"\n",
    "        [Operator]\n",
    "            단일 데이터(sample)에 대해 model을 통과 시키고 검출 결과와 정답 결과를 대응시킨뒤 이를 이미지 리스트 형식으로 출력한다.\n",
    "            이때 검출 여부로 iou_threshold를 고려하며\n",
    "            마스크를 바이너리로 변환 시  mask_binary_threshol를 고려한다.\n",
    "        \"\"\"\n",
    "        sample_result = __class__.eval_sample(model, sample, device)\n",
    "        sample_result = Device.to_for_sample_result(sample_result, \"cpu\")\n",
    "        sample_result = Converter.tensor_to_numpy_for_result(sample_result)\n",
    "        sample_result[\"masks\"] = MH.to_binary_by_threshold(sample_result[\"masks\"], threshold = mask_binary_threshold)\n",
    "        sample = Converter.tensor_to_numpy_for_sample_data(sample)\n",
    "        candidates, targets, iou_list = Evaluator.compare_result_and_gt(sample_result, sample, 21, iou_threshold)\n",
    "\n",
    "        Visualizer.show_comparison_result(candidates, targets, iou_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def eval_sample(model, sample, device = \"cpu\"):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        image, target = Device.to_for_sample_data(sample, device)\n",
    "        result = model([image], [target])[0]\n",
    "        return  Converter.squeeze_dimention_for_sample_result(result)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_eval(model, device, data_loader):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        loss_list = []\n",
    "        for sample in tqdm(data_loader):\n",
    "            images, targets = Device.to_for_batch_data(sample, device)\n",
    "            loss_dict = model(images, targets)\n",
    "            loss_vector = torch.stack([value for value in loss_dict.values()])\n",
    "            loss = loss_vector @ loss_vector**(1/2)\n",
    "            loss_list.append(loss.detach().to(\"cpu\"))\n",
    "            \n",
    "        return np.array(loss_list).mean()\n",
    "        \n",
    "    def map_eval(model, device, data_loader, class_num, mask_threshold, score_threshold):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        counts = []\n",
    "        for images, targets in tqdm(data_loader):\n",
    "            images, targets = Device.to_for_batch_data((images, targets), device)\n",
    "            results = model(images, targets)\n",
    "\n",
    "            # 클래스 별로 GT, TP, Detection 개수 카운트하여 counts에 추가\n",
    "            for result, target in zip(results, targets):\n",
    "\n",
    "                result = Converter.tensor_to_numpy_for_result(result)\n",
    "                target = Converter.tensor_to_numpy_for_target(target)\n",
    "\n",
    "                p_scores, p_labels, p_boxes, p_masks = Converter.dict_to_tuple_for_result(result)\n",
    "                gt_labels, gt_boxes, gt_masks = Converter.dict_to_tuple_for_target(target)\n",
    "\n",
    "                p_masks = MH.to_binary_by_threshold(p_masks, threshold = mask_threshold).squeeze(1)\n",
    "\n",
    "\n",
    "                count_result = Evaluator.count_result(p_masks, p_labels, p_scores, gt_masks, gt_labels, \n",
    "                                            class_num = class_num, \n",
    "                                            score_thresholds = score_threshold,\n",
    "                                            iou_threshold = 0.3)\n",
    "                counts.append(count_result)\n",
    "        \n",
    "        # 카운트 결과 종합\n",
    "        detected_num_list = [count[\"detected_num\"] for count in counts]\n",
    "        gt_num_list = [count[\"gt_num\"] for count in counts]\n",
    "        tp_num_list = [count[\"tp_num\"] for count in counts]\n",
    "\n",
    "        detected_num = np.stack(detected_num_list).sum(0)\n",
    "        gt_num = np.stack(gt_num_list).sum(0)\n",
    "        tp_num = np.stack(tp_num_list).sum(0)\n",
    "\n",
    "        # MAP 계산 후 반환\n",
    "        return Evaluator.ap_per_class(detected_num, gt_num, tp_num)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_sample_by_score(sample_result, threshold):\n",
    "        scores, labels, boxes, masks = __class__.dict_to_tuple_for_result(sample_result)\n",
    "        indexes =  np.where(scores>threshold)[0]\n",
    "        return __class__.tuple_to_dict_for_result(scores[indexes], labels[indexes], boxes[indexes], masks[indexes])\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf1b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Converter:\n",
    "\n",
    "    # dimention\n",
    "    @staticmethod\n",
    "    def squeeze_dimention_for_sample_result(sample_result):\n",
    "        sample_result[\"masks\"] = torch.squeeze(sample_result[\"masks\"], 1)\n",
    "        return sample_result\n",
    "    \n",
    "    def squeeze_dimention_for_batch_result(batch_result):\n",
    "        return [__class__.squeeze_dimention_for_batch_result(x) for x in batch_result]\n",
    "\n",
    "    # tensor numpy\n",
    "    @staticmethod\n",
    "    def tensor_to_numpy(data):\n",
    "        return data.to(\"cpu\").detach().numpy()\n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_to_numpy_for_image(tensor_image):\n",
    "        return __class__.tensor_to_numpy(tensor_image).transpose((1, 2, 0))\n",
    "\n",
    "    @staticmethod \n",
    "    def tensor_to_numpy_for_target(sample_target):\n",
    "        labels, boxes, masks = __class__.dict_to_tuple_for_target(sample_target)\n",
    "        labels = __class__.tensor_to_numpy(labels)\n",
    "        boxes = __class__.tensor_to_numpy(boxes)\n",
    "        masks = __class__.tensor_to_numpy(masks)\n",
    "        return __class__.tuple_to_dict_for_target(labels, boxes, masks)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_to_numpy_for_sample_data(sample_data):\n",
    "        image, target = sample_data\n",
    "        return (__class__.tensor_to_numpy_for_image(image), __class__.tensor_to_numpy_for_target(target))\n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_to_numpy_for_result(sample_result):\n",
    "        scores, labels, boxes, masks = __class__.dict_to_tuple_for_result(sample_result)\n",
    "        scores = __class__.tensor_to_numpy(scores)\n",
    "        labels = __class__.tensor_to_numpy(labels)\n",
    "        boxes = __class__.tensor_to_numpy(boxes)\n",
    "        masks = __class__.tensor_to_numpy(masks)\n",
    "        return __class__.tuple_to_dict_for_result(scores, labels, boxes, masks)\n",
    "    \n",
    "    # dict - tuple\n",
    "    @staticmethod\n",
    "    def dict_to_tuple_for_target(sample_target):\n",
    "        return sample_target[\"labels\"], sample_target[\"boxes\"], sample_target[\"masks\"]\n",
    "    \n",
    "    @staticmethod\n",
    "    def dict_to_tuple_for_result(sample_result):\n",
    "        return  sample_result[\"scores\"], sample_result[\"labels\"], sample_result[\"boxes\"], sample_result[\"masks\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def tuple_to_dict_for_result(scores, labels, boxes, masks):\n",
    "        return {\"scores\":scores, \"labels\":labels, \"boxes\":boxes, \"masks\":masks}\n",
    "    \n",
    "    @staticmethod\n",
    "    def tuple_to_dict_for_target(labels, boxes, masks):\n",
    "        return {\"labels\":labels, \"boxes\":boxes, \"masks\":masks}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    @staticmethod\n",
    "    def score_level(score, thresholds):\n",
    "        f\"\"\"\n",
    "        [Args]\n",
    "            * score: (__float__): 구간을 확인할 score\n",
    "            * thresholds: (__Numpy(N, dtype = float32)__): 객체 검출기의 신뢰도에 대한 임계치 리스트 (내림차순)\n",
    "        [Return]\n",
    "            * result: (__int__): 몇 번째 임계치 보다 큰지 반환\n",
    "        \"\"\"\n",
    "        for level, threshold in enumerate(thresholds):\n",
    "            if score >= threshold:\n",
    "                return level\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_result(p_masks, p_labels, p_scores, gt_masks, gt_labels, class_num, score_thresholds, iou_threshold):\n",
    "        \"\"\"\n",
    "        [Operation]\n",
    "            객체 검출 결과에 대해 클래스 별 detection num, ground truth num, true positive num 개수를 계산한다.\n",
    "\n",
    "        [Return]\n",
    "            * result: (__Dict[str, Any]__):\n",
    "                {\n",
    "                    \"detected_num\": (__Numpy(C, dtype = int)__): 클래스 별로 모델이 검출한 객체 개수\n",
    "                    \"gt_num\": (__Numpy(C, dtype = int)__): 클래스 별 ground truth 객체 개수\n",
    "                    \"tp_num\": (__Numpy(C, S, dtype = int)__): 클래스 및 스코어 구간 별 true positive 개수 (스코어 = 내림차순)\n",
    "                }\n",
    "\n",
    "        \"\"\"\n",
    "        score_levels = [__class__.score_level(score, score_thresholds) for score in p_scores]\n",
    "        \n",
    "        #########################################################################\n",
    "        # prediction과 GT의 idx를 class 별로 구분\n",
    "        # 클래스 별 detected mask indexes\n",
    "        detected_indexes_per_c = [[] for i in range(class_num)]\n",
    "        for idx, label in enumerate(p_labels):\n",
    "            detected_indexes_per_c[label].append(idx)\n",
    "\n",
    "        gt_indexes_per_c = [[] for i in range(class_num)]\n",
    "        for idx, label in enumerate(gt_labels):\n",
    "            gt_indexes_per_c[label].append(idx)\n",
    "            \n",
    "        #########################################################################\n",
    "        # prediction과 GT를 class별로 카운트\n",
    "        \n",
    "        # 클래스 별 탐지한 object 개수\n",
    "        detected_num = np.array([len(x) for x in detected_indexes_per_c])\n",
    "\n",
    "        # 클래스 별 object 개수\n",
    "        gt_num = np.array([len(x) for x in gt_indexes_per_c])\n",
    "        \n",
    "\n",
    "        #########################################################################\n",
    "        \n",
    "\n",
    "        tp_num = np.zeros((class_num, len(score_thresholds)))\n",
    "\n",
    "        for c in range(class_num):\n",
    "            p_idxes = detected_indexes_per_c[c]\n",
    "            gt_idxes = gt_indexes_per_c[c]\n",
    "\n",
    "            if (not p_idxes) or (not gt_idxes):\n",
    "                continue\n",
    "            \n",
    "            for p_idx in p_idxes:\n",
    "                if not gt_idxes: # 탐색할게 없으면 끝\n",
    "                    break\n",
    "\n",
    "                iou_list = [MH.iou(p_masks[p_idx], gt_masks[gt_idx]) for gt_idx in gt_idxes]\n",
    "\n",
    "                max_iou = max(iou_list)\n",
    "                max_idx = np.array(iou_list).argmax()\n",
    "\n",
    "                if iou_threshold <= max_iou:\n",
    "                    tp_num[c][score_levels[p_idx]] += 1\n",
    "                    gt_idxes.pop(max_idx)\n",
    "                    \n",
    "        return {\"detected_num\":detected_num, \"gt_num\":gt_num, \"tp_num\":tp_num}\n",
    "    \n",
    "    @staticmethod\n",
    "    def ap(detected_num, gt_num, tp_num):\n",
    "        \"\"\"\n",
    "        [Operation]\n",
    "            * 단일 클래스에 대해 모델이 검출한 객체 개수, 전체 객체 개수, 신뢰도 별 positive 개수가 주어졌을 때\n",
    "            * average precision을 계산하여 반환한다.\n",
    "        [Args]\n",
    "            * detected_num: (__int__): 전체 데이터 셋에서 클래스에 대해 모델이 제안한 객체 개수\n",
    "            * gt_num: (__int__): 전체 데이터 셋에서 단일 클래스에 대한 ground truth 객체 개수\n",
    "            * tp_num: (__Numpy(S)__): score 구간 1, 0.9, ...., 0 따른 True Positive 개수\n",
    "        [Return]\n",
    "            * result: (__Numpy(1)__) average precision\n",
    "        \"\"\"\n",
    "\n",
    "        # score 구간 별 positive 개수를 score 이상의 positive 개수로 수정 -> 누적\n",
    "        for i in range(0, len(tp_num)-1):\n",
    "            tp_num[i+1] += tp_num[i]\n",
    "\n",
    "        \n",
    "        # score 구간 별 recall 계산 (오른쪽으로 갈 수록 커진다.)\n",
    "        recall = np.ones(tp_num.shape) if gt_num == 0 else (tp_num/gt_num)\n",
    "\n",
    "        # recall 구간 길이 계산\n",
    "        for i in range(1, len(recall)):\n",
    "            recall[i] -= recall[i-1]\n",
    "\n",
    "        precision = np.ones(tp_num.shape) if detected_num == tp_num else np.zeros(tp_num.shape) if detected_num == 0 else (tp_num/detected_num) \n",
    "        \n",
    "        # precision을 계단식으로 맞추기\n",
    "        # recall이 더 높은 영역에서 최대 precision 사용\n",
    "        for i in range(0, len(precision)):\n",
    "            precision[i] = max(precision[i:])\n",
    "        \n",
    "        # 너비 게산\n",
    "        return (recall * precision).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def ap_per_class(detected_num, gt_num, tp_num):\n",
    "        \"\"\"\n",
    "        [Operation]\n",
    "        * 클래스별 \n",
    "        [Args]\n",
    "            * detected_num: (__Numpy(C, dtype = uint8)__):전체 데이터에 대한 class 별 모델의 검출 객체 개수              # C = class num\n",
    "            * gt_num: (__Numpy(C, dtype = uint8)__):전체 데이터에 대한 class 별 ground-truth 객체 개수 \n",
    "            * tp_num: (__Numpy(C, S, dtype = uint8)__):전체 데이터에 대한 class 및 score level 별 true positive 개수    # S = Score level num\n",
    "                - score level 내림차순 (첫 인덱스가 가장 높은 정확도)\n",
    "        [Result]\n",
    "            * result: (__Numpy(C, dtype = float32__): 클래스 별 ap\n",
    "        \"\"\"\n",
    "\n",
    "        ap_list = []\n",
    "        for c in range(len(detected_num)):\n",
    "            ap_value = __class__.ap(detected_num[c], gt_num[c], tp_num[c])\n",
    "            ap_list.append(ap_value)\n",
    "\n",
    "        return np.array(ap_list).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_result_and_gt(result, sample, class_num, iou_threshold):\n",
    "        \"\"\"\n",
    "            [Operation]\n",
    "                iou_threshold를 기준으로 각각의 모델의 객체 검출 결과와 정답을 대응 정보를 반환한다.\n",
    "\n",
    "            [Args]\n",
    "                * result: (__Dict[str, Any]__): MaskRCNN의 단일 출력 데이터\n",
    "                    {\n",
    "                            \"scores\": (__Numpy(M, dtype = float32)__),          # M = detected instance num (candidate num)\n",
    "                            \"labels\": (__Numpy(M, dtype = uint8)__),\n",
    "                            \"boxes\": (__Numpy(M, 4, dtype = float32)__),\n",
    "                            \"masks\": (__Numpy(M, H, W, dtype = float32)__),\n",
    "                    }\n",
    "                * sample: (__Tuple[Image, Target]__): MaskRCNN의 단일 입력 데이터\n",
    "                    - Image: (__Numpy(N, 3, H, W)__)                            # M = target instance num\n",
    "                    - Target: (__Dict[str, Any]__)\n",
    "                        {\n",
    "                            \"labels\": (__Numpy(N, dtype = uint8)__),\n",
    "                            \"boxes\": (__Numpy(N, 4, dtype = float32)__),\n",
    "                            \"masks\": (__Numpy(N, H, W, dtype = float32)__),\n",
    "                        } \n",
    "\n",
    "\n",
    "            [Result]\n",
    "                * result: (__Tuple[Candidates, Targetsm, Iou_List]__): score 순으로 정렬된 candidates\n",
    "                    - Candidates: (__Dict[str, Any]__)\n",
    "                        {\n",
    "                            \"scores\": (__Numpy(M, dtype = float32)__),          # M = detected instance num (candidate num)\n",
    "                            \"labels\": (__Numpy(M, dtype = uint8)__),\n",
    "                            \"boxes\": (__Numpy(M, 4, dtype = float32)__),\n",
    "                            \"masks\": (__Numpy(M, H, W, dtype = float32)__),\n",
    "                        }\n",
    "                    - Targets:  (__Dict[str, Any]__) : candidate에 대응되는 target\n",
    "                        {\n",
    "                            \"labels\": (__Numpy(M, dtype = uint8)__),\n",
    "                            \"boxes\": (__Numpy(M, 4, dtype = float32)__),\n",
    "                            \"masks\": (__Numpy(M, H, W, dtype = float32)__),\n",
    "                        }\n",
    "                    - Iou_List:  (Numpy(N, dtype = float32)__) : candidate와 대응되는 target 간의 iou 리스트\n",
    "        \"\"\"\n",
    "        image, target = sample\n",
    "        \n",
    "        # 탐색 결과와 정답 결과를 쌍 짓기 (누가 무엇을 탐지하는지)\n",
    "        pairs = MH.pair_up_instances(result, target, class_num, iou_threshold)\n",
    "        c_indexes = [pair[0] for pair in pairs] # detection 결과 idx 리스트\n",
    "        t_indexes = [pair[1] for pair in pairs] # 대응되는 target(gt)의 idx 리스트 (없으면 -1)\n",
    "\n",
    "        c_scores, c_labels, c_boxes, c_masks = Converter.dict_to_tuple_for_result(result)\n",
    "        t_labels, t_boxes, t_masks = Converter.dict_to_tuple_for_target(target)\n",
    "\n",
    "        # target = -1 인 경우에 대한 값 추가\n",
    "        t_labels = np.append(t_labels, [0], axis = 0) # 배경 추가\n",
    "        t_boxes = np.append(t_boxes, [[-1, -1, -1, -1]], axis = 0)\n",
    "        t_masks = np.append(t_masks, [np.zeros(t_masks[0].shape)], axis = 0)\n",
    "\n",
    "        # candidate 정보를 candidate indexes에 맞게 순서 정리\n",
    "        c_scores, c_labels, c_boxes, c_masks = c_scores[c_indexes], c_labels[c_indexes], c_boxes[c_indexes], c_masks[c_indexes]\n",
    "\n",
    "        # gt 정보를 candidate의 대응되는 idx에 따라 정리\n",
    "        t_labels, t_boxes, t_masks = t_labels[t_indexes], t_boxes[t_indexes], t_masks[t_indexes]\n",
    "\n",
    "        candidates = {\n",
    "            \"scores\" : c_scores,\n",
    "            \"labels\" : c_labels,\n",
    "            \"boxes\" : c_boxes,\n",
    "            \"masks\" : c_masks,\n",
    "        }\n",
    "\n",
    "        targets = {\n",
    "            \"labels\" : t_labels,\n",
    "            \"boxes\" : t_boxes,\n",
    "            \"masks\" : t_masks\n",
    "        }\n",
    "\n",
    "        iou_list = np.array([MH.iou(c, t) for c, t in zip(c_masks, t_masks)])\n",
    "\n",
    "        return (candidates, targets, iou_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04caf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0405107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    def show_comparison_result(candidates, targets, iou_list):\n",
    "        \"\"\"\n",
    "            검출 결과 (candidate)와 그에 대응되는 정답 객체(target)를 나란히 출력한다.\n",
    "        \"\"\"\n",
    "        c_scores, c_labels, c_boxes, c_masks = candidates[\"scores\"], candidates[\"labels\"], candidates[\"boxes\"], candidates[\"masks\"]\n",
    "        t_labels, t_boxes, t_masks = targets[\"labels\"], targets[\"boxes\"], targets[\"masks\"]\n",
    "        \n",
    "        n = len(c_scores)\n",
    "        fig = plt.figure(figsize = (6, 3*n))\n",
    "\n",
    "\n",
    "        for i, (c_score, c_label, c_box, c_mask, t_label, t_box, t_mask, iou) in enumerate(\n",
    "            zip(c_scores, c_labels, c_boxes, c_masks, t_labels, t_boxes, t_masks, iou_list)\n",
    "        ):\n",
    "            # prediction instance\n",
    "            sub = fig.add_subplot(n, 2, 2*i+1)\n",
    "            sub.imshow(c_mask)\n",
    "            sub.set_title(\"Prediction(%s, %.3f, %.3f)\"%(VOC_LABEL[c_label], c_score, iou), fontdict={\"fontsize\":8})\n",
    "\n",
    "            # Corresponding instance\n",
    "            sub = fig.add_subplot(n, 2, 2*i+2)\n",
    "            sub.imshow(t_mask)\n",
    "            sub.set_title(f\"GT({VOC_LABEL[t_label]})\", fontdict={\"fontsize\":8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d60843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ada6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train()\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    losses_list = []\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            loss_list = torch.stack([value for value in loss_dict.values()])\n",
    "            losses = loss_list @ loss_list**(1/2)\n",
    "            losses_list.append(losses)\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    return torch.mean(torch.stack(losses_list)).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6000c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    losses_list = []\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        # to device\n",
    "        images, targets = Device.to_for_batch_data((images, targets), device)\n",
    "\n",
    "        # model\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # loss\n",
    "        loss_list = torch.stack([value for value in loss_dict.values()])\n",
    "        losses = loss_list @ loss_list**(1/2)\n",
    "        losses_list.append(losses)\n",
    "\n",
    "        # back propergation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    return torch.mean(torch.stack(losses_list)).to(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70d3f36e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
